import os
import json
from config import *
import vertexai
from vertexai.generative_models import GenerativeModel
import vertexai.preview.generative_models as generative_models
import jsonschema
import utils

class ExtractConfigClient:
    def __init__(self):
        """Initializes the ExtractConfigClient with the Gemini model."""
        self.schema = load_activity_config_schema()
        vertexai.init(project=PROJECT_ID, location=LOCATION)
        self.model = GenerativeModel(os.getenv("GEMINI_MODEL"),
            system_instruction="""You are an expert in extracting configuration details for an e-learning platform from text. You will be given a JSON configuration schema, text input from an educator containing data to extract, and the current JSON configuration to be modified."""
        )

    def extract_values(self, prev_system_response, user_input, current_config):
        """Extracts configuration values from the given text based on the structure.

        Args:
            text: The text input describing the configuration.
            current_config: The current configuration structure.

        Returns:
            The extracted configuration as a JSON object.
        """

        print(f"Processing, please wait...")

        payload = {
            "config_schema": self.schema,
            "prev_system_response": prev_system_response,
            "user_input": user_input,
            "current_config": current_config
        }

        json_payload = json.dumps(payload, indent=2)
        prompt = (
            "The following is a JSON object containing the schema, the user input text, and the current configuration:\n\n"
            f"{json_payload}\n\n"
            "1. Extract the relevant information from the user input text, given in response to the 'prev_system_response' text.\n"
            f"2. If changes were proposed, update the current configuration using the extracted values, otherwise, keep the current config 'as is'. Smaller y values represent the front of the classroom.\n"
            "3. Return the configuration as a JSON object, ensuring the JSON is valid and does not contain any extra characters or formatting.\n"
        )

        # Call the model and process response
        generation_config = {
            "max_output_tokens": 1024,
            "temperature": 0.2,
            "top_p": 0.95,
            "top_k": 40
        }

        safety_settings = {
            generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
            generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
            generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
            generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
        }
        
        # Call the model to predict and get results in string format
        response = self.model.generate_content(prompt, generation_config=generation_config, safety_settings=safety_settings).text
        clean_response = utils.remove_json_markdown(response)

        # Validate response against schema
        try:
            jsonschema.validate(instance=json.loads(clean_response), schema=self.schema)
        except jsonschema.exceptions.ValidationError as e:
            if "[] should be non-empty" not in str(e): # Don't throw an error if definition is still incomplete (only if incorrect)
                raise ValueError(f"Invalid configuration generated by LLM: {e}")

        return json.loads(clean_response)
